{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b336658-a87a-45d3-a2e5-0fc41a3fc87b",
   "metadata": {},
   "source": [
    "<font size=5>**神经网络背向传播的矩阵形式普适描述（含多样本与梯度下降方法）**</font>\n",
    "\n",
    "本文档以矩阵形式描述具有任意 $L$ 层（包括输入层、若干隐藏层和输出层）的全连接神经网络的前向传播和后向传播过程，涵盖多样本场景下的矩阵维度、运算规则和通用的背向传播算法。特别地，描述包括批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent, SGD）和小批量梯度下降（Mini-batch Gradient Descent）的实现方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11602f66-205c-48a3-8769-cacf1af2f9e6",
   "metadata": {},
   "source": [
    "# 1. 网络结构与符号定义\n",
    "\n",
    "- 层数：网络共有 $L$ 层，从 $l = 0$（输入层）到 $l = L-1$（输出层）。层 $l = 1, \\dots, L-2$ 为隐藏层。\n",
    "- 每层神经元数：第 $l$ 层有 $n_l$ 个神经元（输入层为特征数）。\n",
    "- 样本数：训练数据集包含 $N$ 个样本。对于批量处理，定义批量大小为 $B$（$1 \\leq B \\leq N$）。\n",
    "- 输入：输入矩阵 $\\boldsymbol{X} = [\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_B] = \\boldsymbol{A}^{(0)}$，维度 $n_0 \\times B$，每列 $\\boldsymbol{x}_i$ 为一个样本（维度 $n_0 \\times 1$）。\n",
    "- 输出：预测输出矩阵 $\\hat{\\boldsymbol{Y}} = \\boldsymbol{A}^{(L-1)}$，维度 $n_{L-1} \\times B$，每列 $\\hat{\\boldsymbol{y}}i$ 为一个样本的预测（维度 $n_{L-1} \\times 1$）。\n",
    "- 真实标签：标签矩阵 $\\boldsymbol{Y} = [\\boldsymbol{y}_1, \\boldsymbol{y}_2, \\dots, \\boldsymbol{y}_B]$，维度 $n_{L-1} \\times B$，每列 $\\boldsymbol{y}i$ 为一个样本的标签（维度 $n_{L-1} \\times 1$）。\n",
    "- 权重：从第 $l-1$ 层到第 $l$ 层的权重矩阵 $\\boldsymbol{W}^{(l)}$，维度 $n_l \\times n_{l-1}$。\n",
    "- 偏置：第 $l$ 层的偏置向量 $\\boldsymbol{b}^{(l)}$，维度 $n_l \\times 1$。\n",
    "- 预激活值：第 $l$ 层的输入矩阵 $\\boldsymbol{Z}^{(l)}$，维度 $n_l \\times B$，每列为一个样本的预激活值。\n",
    "- 激活值：第 $l$ 层的输出矩阵 $\\boldsymbol{A}^{(l)} = \\sigma^{(l)}(\\boldsymbol{Z}^{(l)})$，维度 $n_l \\times B$，每列为一个样本的激活值。\n",
    "- 激活函数：$\\sigma^{(l)}$，逐元素应用（如 sigmoid、ReLU，输出层可能为恒等函数）。\n",
    "- 损失函数：均方误差（MSE），对 $B$ 个样本求平均：\n",
    "\n",
    "$$L = \\frac{1}{2B} \\sum_{i=1}^B ||\\boldsymbol{y}_i - \\hat{\\boldsymbol{y}}_i||_2^2 = \\frac{1}{2B} \\sum_{i=1}^B \\sum_{j=1}^{n_{L-1}} (y_{ji} - \\hat{y}_{ji})^2$$\n",
    "\n",
    "- 逐元素乘法：用 $\\odot$ 表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8206e3-c3c9-4f80-99f5-bdcc25935d6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. 前向传播（多样本）\n",
    "\n",
    "前向传播对批量输入 $\\boldsymbol{X}$（$n_0 \\times B$）逐层计算输出。\n",
    "对每一层 $l = 1, 2, \\dots, L-1$：\n",
    "\n",
    "- 1. 预激活值：$$\\boldsymbol{Z}^{(l)} = \\boldsymbol{W}^{(l)} \\boldsymbol{A}^{(l-1)} + \\boldsymbol{b}^{(l)} \\boldsymbol{1}_B^\\top$$\n",
    "\n",
    "    - 维度：\n",
    "        - $\\boldsymbol{W}^{(l)}$：$n_l \\times n_{l-1}$\n",
    "        - $\\boldsymbol{A}^{(l-1)}$：$n_{l-1} \\times B$\n",
    "        - $\\boldsymbol{W}^{(l)} \\boldsymbol{A}^{(l-1)}$：$(n_l \\times n_{l-1}) \\cdot (n_{l-1} \\times B) = n_l \\times B$\n",
    "        - $\\boldsymbol{b}^{(l)}$：$n_l \\times 1$\n",
    "        - $\\boldsymbol{1}_B$：$B \\times 1$（全 1 向量）\n",
    "        - $\\boldsymbol{b}^{(l)} \\boldsymbol{1}_B^\\top$：$n_l \\times B$（偏置广播到每个样本）\n",
    "        - 结果 $\\boldsymbol{Z}^{(l)}$：$n_l \\times B$\n",
    "    \n",
    "    - 说明：矩阵乘法对批量样本并行计算，偏置通过广播加到每个样本。\n",
    "\n",
    "- 2. 激活值：$$\\boldsymbol{A}^{(l)} = \\sigma^{(l)}(\\boldsymbol{Z}^{(l)})$$\n",
    "\n",
    "    - 维度：$\\boldsymbol{A}^{(l)}$：$n_l \\times B$\n",
    "    - 说明：激活函数逐元素作用于矩阵，维度不变。\n",
    "\n",
    "- 3. 输出层：\n",
    "\n",
    "    - 最后一层 $l = L-1$ 的输出为 $\\hat{\\boldsymbol{Y}} = \\boldsymbol{A}^{(L-1)}$。\n",
    "    - 计算损失：$$L = \\frac{1}{2B} ||\\boldsymbol{Y} - \\boldsymbol{A}^{(L-1)}||_F^2$$（$||\\cdot||_F$ 为 Frobenius 范数）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74a23a-cd0a-47cd-bf99-422e4890321c",
   "metadata": {},
   "source": [
    "# 3. 后向传播（多样本）\n",
    "\n",
    "后向传播计算损失对每一层权重和偏置的梯度，考虑批量样本的平均梯度。\n",
    "\n",
    "### 3.1. 输出层误差项\n",
    "\n",
    "对输出层 $l = L-1$，定义误差项矩阵：$$\\boldsymbol{\\Delta}^{(L-1)} = \\frac{\\partial L}{\\partial \\boldsymbol{A}^{(L-1)}} \\odot \\sigma^{(L-1)'}(\\boldsymbol{Z}^{(L-1)})$$\n",
    "\n",
    "- 计算：\n",
    "损失对输出的梯度：$$\\frac{\\partial L}{\\partial \\boldsymbol{A}^{(L-1)}} = - \\frac{1}{B} (\\boldsymbol{Y} - \\boldsymbol{A}^{(L-1)})$$\n",
    "    - 维度：$n_{L-1} \\times B$\n",
    "    - 说明：因子 $\\frac{1}{B}$ 确保批量样本的损失平均。\n",
    "\n",
    "\n",
    "- 激活函数导数：$$\\sigma^{(L-1)'}(\\boldsymbol{Z}^{(L-1)})$$\n",
    "    - 维度：$n_{L-1} \\times B$\n",
    "\n",
    "\n",
    "- 误差项：$$\\boldsymbol{\\Delta}^{(L-1)} = - \\frac{1}{B} (\\boldsymbol{Y} - \\boldsymbol{A}^{(L-1)}) \\odot \\sigma^{(L-1)'}(\\boldsymbol{Z}^{(L-1)})$$\n",
    "    - 维度：$n_{L-1} \\times B$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b4ee8f-72f2-45d1-8424-01f79118648c",
   "metadata": {},
   "source": [
    "### 3.2. 隐藏层误差项\n",
    "\n",
    "对每一隐藏层 $l = L-2, L-3, \\dots, 1$，误差项矩阵递归计算：$$\\boldsymbol{\\Delta}^{(l)} = [(\\boldsymbol{W}^{(l+1)})^\\top \\boldsymbol{\\Delta}^{(l+1)}] \\odot \\sigma^{(l)'}(\\boldsymbol{Z}^{(l)})$$\n",
    "\n",
    "- 计算：\n",
    "    - 误差传播：$$(\\boldsymbol{W}^{(l+1)})^\\top \\boldsymbol{\\Delta}^{(l+1)}$$\n",
    "    - 维度：\n",
    "        - $\\boldsymbol{W}^{(l+1)}$：$n_{l+1} \\times n_l$\n",
    "        - $(\\boldsymbol{W}^{(l+1)})^\\top$：$n_l \\times n_{l+1}$\n",
    "        - $\\boldsymbol{\\Delta}^{(l+1)}$：$n_{l+1} \\times B$\n",
    "        - 结果：$(n_l \\times n_{l+1}) \\cdot (n_{l+1} \\times B) = n_l \\times B$\n",
    "\n",
    "- 激活函数导数：$$\\sigma^{(l)'}(\\boldsymbol{Z}^{(l)})$$\n",
    "    - 维度：$n_l \\times B$\n",
    "\n",
    "\n",
    "- 误差项：$$\\boldsymbol{\\Delta}^{(l)} = [(\\boldsymbol{W}^{(l+1)})^\\top \\boldsymbol{\\Delta}^{(l+1)}] \\odot \\sigma^{(l)'}(\\boldsymbol{Z}^{(l)})$$\n",
    "    - 维度：$n_l \\times B$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b3fee-7e7c-491e-bb91-0441ce3d6310",
   "metadata": {},
   "source": [
    "### 3.3. 权重和偏置的梯度\n",
    "对每一层 $l = 1, 2, \\dots, L-1$：\n",
    "\n",
    "- 权重梯度：$$\\frac{\\partial L}{\\partial \\boldsymbol{W}^{(l)}} = \\boldsymbol{\\Delta}^{(l)} (\\boldsymbol{A}^{(l-1)})^\\top$$\n",
    "\n",
    "    - 维度：\n",
    "        - $\\boldsymbol{\\Delta}^{(l)}$：$n_l \\times B$\n",
    "        - $\\boldsymbol{A}^{(l-1)}$：$n_{l-1} \\times B$\n",
    "        - $(\\boldsymbol{A}^{(l-1)})^\\top$：$B \\times n_{l-1}$\n",
    "        - 结果：$(n_l \\times B) \\cdot (B \\times n_{l-1}) = n_l \\times n_{l-1}$\n",
    "    \n",
    "    - 说明：矩阵乘法对批量样本的梯度求和，生成与 $\\boldsymbol{W}^{(l)}$ 相同维度的梯度矩阵。\n",
    "\n",
    "\n",
    "- 偏置梯度：$$\\frac{\\partial L}{\\partial \\boldsymbol{b}^{(l)}} = \\boldsymbol{\\Delta}^{(l)} \\boldsymbol{1}_B$$\n",
    "\n",
    "    - 维度：\n",
    "        - $\\boldsymbol{\\Delta}^{(l)}$：$n_l \\times B$\n",
    "        - $\\boldsymbol{1}_B$：$B \\times 1$\n",
    "        - 结果：$(n_l \\times B) \\cdot (B \\times 1) = n_l \\times 1$\n",
    "\n",
    "- 说明：对批量样本的误差求和，生成偏置梯度。\n",
    "\n",
    "### 3.4. 参数更新\n",
    "\n",
    "使用梯度下降更新参数：\n",
    "\n",
    "$$\\boldsymbol{W}^{(l)} \\leftarrow \\boldsymbol{W}^{(l)} - \\eta \\frac{\\partial L}{\\partial \\boldsymbol{W}^{(l)}}$$\n",
    "\n",
    "$$\\boldsymbol{b}^{(l)} \\leftarrow \\boldsymbol{b}^{(l)} - \\eta \\frac{\\partial L}{\\partial \\boldsymbol{b}^{(l)}}$$\n",
    "\n",
    "其中，$\\eta$ 为学习率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a593499-8bbb-4002-901e-79e24d8c49a7",
   "metadata": {},
   "source": [
    "# 4. 梯度下降方法\n",
    "\n",
    "在多样本场景下，梯度下降方法根据批量大小 $B$ 分为以下三种：\n",
    "\n",
    "### 4.1 批量梯度下降（Batch Gradient Descent）：\n",
    "\n",
    "- 批量大小：$B = N$（使用全部训练样本）。\n",
    "- 特点：\n",
    "    - 计算所有样本的平均梯度，更新参数：\n",
    "      $$\\frac{\\partial L}{\\partial \\boldsymbol{W}^{(l)}} = \\frac{1}{N} \\boldsymbol{\\Delta}^{(l)} (\\boldsymbol{A}^{(l-1)})^\\top, \\quad \\frac{\\partial L}{\\partial \\boldsymbol{b}^{(l)}} = \\frac{1}{N} \\boldsymbol{\\Delta}^{(l)} \\boldsymbol{1}_N$$\n",
    "    - 梯度估计精确，但计算成本高，内存需求大。\n",
    "\n",
    "- 适用场景：数据集较小，或需要稳定的梯度方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fc475c-8b64-4120-828b-8542b6d5c956",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.2 随机梯度下降（Stochastic Gradient Descent, SGD）：\n",
    "\n",
    "- 批量大小：$B = 1$（每次随机选择一个样本）。\n",
    "- 特点：\n",
    "    - 对单个样本计算梯度，更新参数：\n",
    "      $$\\frac{\\partial L}{\\partial \\boldsymbol{W}^{(l)}} = \\boldsymbol{\\delta}^{(l)} (\\boldsymbol{a}^{(l-1)})^\\top, \\quad \\frac{\\partial L}{\\partial \\boldsymbol{b}^{(l)}} = \\boldsymbol{\\delta}^{(l)}$$\n",
    "      （$\\boldsymbol{\\delta}^{(l)}$ 为单样本误差项，维度 $n_l \\times 1$）。\n",
    "    - 梯度噪声大，更新频繁，可能逃离局部最小值，但收敛路径波动。\n",
    "\n",
    "- 适用场景：大数据集，需快速迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d5f26e-0b4b-4da3-ae3a-9ed27135fc7f",
   "metadata": {},
   "source": [
    "### 4.3 小批量梯度下降（Mini-batch Gradient Descent）：\n",
    "\n",
    "- 批量大小：$1 < B < N$（每次随机选择一小批样本）。\n",
    "- 特点：\n",
    "    - 对批量样本计算平均梯度，更新参数，如 3.3 节所述。\n",
    "    - 平衡了批量梯度下降的稳定性和随机梯度下降的计算效率。\n",
    "    - 批量大小 $B$ 通常为 32、64、128 等（视硬件和任务调整）。\n",
    "\n",
    "- 适用场景：现代深度学习中最常用的方法，适合大数据集和 GPU 并行计算。\n",
    "\n",
    "### 4.4 批量选择与实现：\n",
    "\n",
    "- 在训练中，数据集通常被划分为多个小批量（mini-batches）。每个 epoch 遍历所有小批量，随机打乱样本顺序以减少偏差。\n",
    "- 矩阵运算（尤其 $\\boldsymbol{W}^{(l)} \\boldsymbol{A}^{(l-1)}$ 和 $\\boldsymbol{\\Delta}^{(l)} (\\boldsymbol{A}^{(l-1)})^\\top$）天然支持批量处理，适合 GPU 加速。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb9522-a5ea-4d43-97d3-de6cf1064c06",
   "metadata": {},
   "source": [
    "# 5. 算法流程\n",
    "\n",
    "- 1. 初始化：随机初始化所有 $\\boldsymbol{W}^{(l)}$ 和 $\\boldsymbol{b}^{(l)}$。\n",
    "- 2. 数据准备：将数据集划分为小批量（批量大小 $B$），随机打乱样本顺序。\n",
    "- 3. 对每个 epoch：\n",
    "    - 对每个小批量（$\\boldsymbol{X}$, $\\boldsymbol{Y}$，维度 $n_0 \\times B$ 和 $n_{L-1} \\times B$）：\n",
    "    - 前向传播：\n",
    "        - 对 $l = 1, \\dots, L-1$，计算：$$\\boldsymbol{Z}^{(l)} = \\boldsymbol{W}^{(l)} \\boldsymbol{A}^{(l-1)} + \\boldsymbol{b}^{(l)} \\boldsymbol{1}_B^\\top, \\quad \\boldsymbol{A}^{(l)} = \\sigma^{(l)}(\\boldsymbol{Z}^{(l)})$$\n",
    "        - 计算损失 $L$。\n",
    "\n",
    "    - 后向传播：\n",
    "        - 计算输出层误差项：\n",
    "          $$\\boldsymbol{\\Delta}^{(L-1)} = - \\frac{1}{B} (\\boldsymbol{Y} - \\boldsymbol{A}^{(L-1)}) \\odot \\sigma^{(L-1)'}(\\boldsymbol{Z}^{(L-1)})$$\n",
    "        - 对 $l = L-2, \\dots, 1$，递归计算：\n",
    "          $$\\boldsymbol{\\Delta}^{(l)} = [(\\boldsymbol{W}^{(l+1)})^\\top \\boldsymbol{\\Delta}^{(l+1)}] \\odot \\sigma^{(l)'}(\\boldsymbol{Z}^{(l)})$$\n",
    "        - 计算梯度：\n",
    "          $$\\frac{\\partial L}{\\partial \\boldsymbol{W}^{(l)}} = \\boldsymbol{\\Delta}^{(l)} (\\boldsymbol{A}^{(l-1)})^\\top, \\quad \\frac{\\partial L}{\\partial \\boldsymbol{b}^{(l)}} = \\boldsymbol{\\Delta}^{(l)} \\boldsymbol{1}_B$$\n",
    "\n",
    "    - 更新参数：按选定的梯度下降方法（批量、随机或小批量）更新 $\\boldsymbol{W}^{(l)}$ 和 $\\boldsymbol{b}^{(l)}$。\n",
    "\n",
    "- 4. 迭代：重复 epoch，直到损失收敛或达到最大迭代次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fefcbe5-7a1d-48b2-b1b8-3ca6115eb297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 导入类库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "# 定义类\n",
    "class MultilayerPerceptron:\n",
    "    \"\"\"\n",
    "    这是一个多层感知机类\n",
    "\n",
    "    属性：\n",
    "        data：输入数据，要求是一个大小为 n0xN 的 array 数组，其中 N 表示样本数，n0 表示变量数。\n",
    "            对于每一个样本，输入都是一个列向量，与通常的机器学习样本不同\n",
    "        labels：数据标签，要求是一个大小为 n_{L:-1}xN 的 array 数组，其中 n_{L:-1} 表示第 (L-1) 层的\n",
    "            变量数，N 表示样本数\n",
    "        size：神经网络各层大小，是一个 list，size = [n0, n1, ..., n_{L-1}]\n",
    "        num_sample：样本数\n",
    "        \n",
    "    方法：\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, data, labels, size, normlized=True):\n",
    "        \"\"\"\n",
    "        初始化 MultilayerPerceptron 类的实例\n",
    "\n",
    "        参数：\n",
    "            data: 输入数据，要求是一个大小为 mxn0 的 array 数组，其中 m 表示样本数，n0 表示变量数\n",
    "            labels: 数据标签，要求是一个大小为 mx1 的 array 数组，其中 m 表示样本数\n",
    "            size: 神经网络各层大小，是一个 list，size = [n0, n1, ..., n_{L-1}]\n",
    "            normlized: 是否归一化，默认为 True\n",
    "        \"\"\"\n",
    "        W, b = self.theta_init(size) \n",
    "        self.W, self.b = W, b # 参数初始化\n",
    "        \n",
    "        if normlized:\n",
    "            data = data/255.\n",
    "        \n",
    "        self.normlized = normlized\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.size = size\n",
    "        self.num_sample = data.shape[0] \n",
    "        \n",
    "\n",
    "    def train(self, epoch=1000, eta=0.1, batch_size = 128, test_data=None):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "\n",
    "        参数：\n",
    "            epoch: 循环次数。默认为 1000\n",
    "            eta: 学习率，用于更新权重系数。默认为 0.1\n",
    "            batch_size: 小批量梯度下降的样本大小，默认为 128.当此值为 1 时，对应随机梯度下降，\n",
    "                        当此值与样本数相同时，对应批量梯度下降\n",
    "            test_data: 测试集，不拆分标签和变量，大小为 (m+1)xn 的 narray。其中 m 是变量数，\n",
    "                        m+1 是数据标签，n 是样本数。默认为 None，即不测试。     \n",
    "\n",
    "        返回：\n",
    "            W：权重矩阵，是一个长度为 L-1 的字典，其中 W[l] 表示第 l-1 到第 l 层传播时的权重矩阵\n",
    "            b：层间传播的偏置项，是一个长度为 L-1 的字典，其中 b[l] 表示第 l-1 层到第 l 层传播时的偏置系数\n",
    "            loss：损失函数\n",
    "        \"\"\"\n",
    "        W, b = self.W, self.b\n",
    "        loss = []\n",
    "        if test_data is not None: train_score = []\n",
    "        plt.ion()\n",
    "        \n",
    "        for epoch_index in range(epoch):\n",
    "            shuffled_index = np.random.permutation(self.data.shape[1])\n",
    "            shuffled_data = [self.data[:, shuffled_index[ii:ii+batch_size]] \n",
    "                             for ii in range(0, shuffled_index.size, batch_size)]\n",
    "            shuffled_labels = [self.labels[shuffled_index[ii:ii+batch_size]] \n",
    "                               for ii in range(0, shuffled_index.size, batch_size)]\n",
    "            \n",
    "            for mini_data, mini_labels in list(zip(shuffled_data, shuffled_labels)):\n",
    "            \n",
    "                # 前向传播\n",
    "                Z,A,_ = self.forward_propagation(mini_data, W, b)\n",
    "                \n",
    "                # 后向传播\n",
    "                Delta = self.back_propagation(mini_labels, W, Z, A)\n",
    "                \n",
    "                # 参数更新\n",
    "                for layer_index in np.arange(len(W))+1:\n",
    "                    W[layer_index] -= eta*Delta[layer_index]@A[layer_index-1].T\n",
    "                    b[layer_index] -= eta*Delta[layer_index]@np.ones((Delta[layer_index].shape[1],1))\n",
    "                \n",
    "            if test_data is not None:\n",
    "                train_score.append(self.test_score(test_data, W, b))\n",
    "                plt.clf()\n",
    "                plt.plot(train_score)\n",
    "                plt.xlabel('Epoches')\n",
    "                plt.ylabel('Accuracy Rate')\n",
    "                plt.xlim((0, epoch))\n",
    "                plt.pause(0.01)\n",
    "            else:\n",
    "                loss.append(MultilayerPerceptron.get_mse(self.data, W, b, self.labels, size))\n",
    "                plt.clf()\n",
    "                plt.plot(loss)\n",
    "                plt.xlabel('Epoches')\n",
    "                plt.ylabel('MSE')\n",
    "                plt.xlim((0, epoch))\n",
    "                plt.pause(0.01)\n",
    "                         \n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        self.W, self.b = W, b      \n",
    "        \n",
    "\n",
    "    @staticmethod  \n",
    "    def get_mse(data, W, b, labels, size):\n",
    "        \"\"\"\n",
    "        训练集上的均方差\n",
    "        \n",
    "        变量:\n",
    "            data: 抽选出来的样本值\n",
    "            W: 初始化的权重矩阵，是一个长度为 L-1 的字典，其中 W[l] 表示第 l-1 到第 l 层传播时的权重矩阵\n",
    "            b：初始化的层间传播的偏置项，是一个长度为 L-1 的字典，其中 b[l] 表示第 l-1 层到第 l 层传播时的偏置系数\n",
    "            labels: Bx1 大小的 narray\n",
    "        返回：\n",
    "            mse: 均方差\n",
    "        \"\"\"\n",
    "        \n",
    "        one_hot_labels = MultilayerPerceptron.one_hot(labels)\n",
    "        B = labels.size\n",
    "        _, _, Y_hat = MultilayerPerceptron.forward_propagation(data, W, b)\n",
    "        mse = np.sum((Y_hat - one_hot_labels)**2)/B\n",
    "        \n",
    "        return mse\n",
    "    \n",
    "    \n",
    "    def test_score(self, test_data, W, b):\n",
    "        \"\"\"\n",
    "        训练集上的均方差\n",
    "        \n",
    "        变量:\n",
    "            test_data: 测试集，不拆分标签和变量，大小为 (m+1)xn 的 narray。其中 m 是变量数，\n",
    "                        m+1 是数据标签，n 是样本数。默认为 None，即不测试。     \n",
    "            W: 训练过程中的权重值\n",
    "            b：训练过程中的标签值\n",
    "        返回：\n",
    "            score: 在测试集上的预测准确率\n",
    "        \"\"\"\n",
    "        \n",
    "        Y = test_data[0,:]\n",
    "        B = test_data.shape[1]\n",
    "        if self.normlized:\n",
    "            test_data = test_data[1:,:]/255.0\n",
    "        else:\n",
    "            test_data = test_data[1:,:]\n",
    "        _, _, Y_hat = MultilayerPerceptron.forward_propagation(test_data, W, b)\n",
    "        Y_predict = np.argmax(Y_hat, axis=0)\n",
    "        score = np.sum(Y==Y_predict)/B\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    \n",
    "    @staticmethod  \n",
    "    def one_hot(labels):\n",
    "        \"\"\"\n",
    "        将 0-9 之间的数字标签转换为 one-hot 格式的标签\n",
    "        \n",
    "        变量:\n",
    "            labels: Bx1 大小的 narray\n",
    "            \n",
    "        返回：\n",
    "            one_hot_labels: 10xB 大小的 narray\n",
    "        \"\"\"\n",
    "        \n",
    "        B = labels.size\n",
    "        one_hot_labels = np.zeros((10, B))\n",
    "        \n",
    "        for sample_index in range(B):\n",
    "            one_hot_labels[labels[sample_index], sample_index] = 1\n",
    "            \n",
    "        return one_hot_labels\n",
    "\n",
    "\n",
    "    @staticmethod  \n",
    "    def back_propagation(labels, W, Z, A):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "    \n",
    "        参数：\n",
    "            labels: 输入数据的标签，大小为 n_0xB 的 narray 结构。对于数字识别问题，标签是一个数字。\n",
    "            W: 初始化的权重矩阵，是一个长度为 L-1 的字典，其中 W[l] 表示第 l-1 到第 l 层传播时的权重矩阵\n",
    "            Z：字典类型，长度为 (L-1)，Z[l] 里面是第 l 层的输入值，是一个大小为 n_lxB 的 narray 格式的数组\n",
    "            A：字典类型，长度为 (L-1)，A[l] 里面是第 l 层输入值经过激活函数的非线性变换后的输出值，是一个大小\n",
    "                为 n_lxB 的 narray 格式的数组\n",
    "\n",
    "        返回：\n",
    "            Delta: 字典类型，长度为 (L-1)，Delta[l] 里面是第 l 层的误差矩阵，对应损失函数对第 l 层输入值 Z_l \n",
    "                的偏导 \\frac{\\partial L}{\\partial Z}，大小为 n_lxB\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        Y = MultilayerPerceptron.one_hot(labels) # 将 labels 转为 one-hot 格式\n",
    "        Delta, batch_size = {}, labels.size\n",
    "        for layer_index in np.arange(len(W), 0, -1):\n",
    "            if layer_index == len(W): # 最后一层\n",
    "                Delta[layer_index] = -(Y - A[layer_index]) \\\n",
    "                *MultilayerPerceptron.sigmoid_gradient(Z[layer_index])/batch_size\n",
    "            else:\n",
    "                Delta[layer_index] = W[layer_index+1].T@Delta[layer_index+1] \\\n",
    "                *MultilayerPerceptron.sigmoid_gradient(Z[layer_index])\n",
    "\n",
    "        return Delta\n",
    "     \n",
    "    \n",
    "    @staticmethod            \n",
    "    def forward_propagation(data, W, b):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        参数：\n",
    "            data：输入数据，要求是一个大小为 mxn0 的 array 数组，其中 m 表示样本数，n0 表示变量数\n",
    "            W：初始化的权重矩阵，是一个长度为 L-1 的字典，其中 W[l] 表示第 l-1 到第 l 层传播时的权重矩阵\n",
    "            b：初始化的层间传播的偏置项，是一个长度为 L-1 的字典，其中 b[l] 表示第 l-1 层到第 l 层传播时的偏置系数\n",
    "            size：神经网络各层大小，是一个 list，size = [n0, n1, ..., n_{L-1}]\n",
    "\n",
    "        返回：\n",
    "            Z：字典类型，长度为 L。其中, Z[0] 即是输入数据。Z[l] 里面是第 l 层的输入值，是一个大小为 n_lxB 的 narray 格式的数组\n",
    "            A：字典类型，长度为 L。其中, A[0] 即是输入数据。A[l] 里面是第 l 层输入值经过激活函数的非线性变换后的输出值，是一个大小\n",
    "                为 n_lxB 的 narray 格式的数组\n",
    "        \"\"\"\n",
    "        Z, A = {}, {}\n",
    "        Z[0] = data.copy()\n",
    "        A[0] = data.copy()\n",
    "        for layer_index in np.arange(len(W))+1:\n",
    "            Z[layer_index] = W[layer_index]@A[layer_index-1] + b[layer_index]\n",
    "            A[layer_index] = MultilayerPerceptron.sigmoid(Z[layer_index])\n",
    "        \n",
    "        return Z, A, A[len(W)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def theta_init(size):\n",
    "        \"\"\"\n",
    "        参数初始化\n",
    "\n",
    "        参数：\n",
    "            size：模型的各层参数\n",
    "\n",
    "        返回：\n",
    "            W：初始化的权重矩阵，是一个长度为 L-1 的字典，其中 W[l] 表示第 l-1 到第 l 层传播时的权重矩阵\n",
    "            b：初始化的层间传播的偏置项，是一个长度为 L-1 的字典，其中 b[l] 表示第 l-1 层到第 l 层传播时的偏置系数\n",
    "        \"\"\"\n",
    "        W, b = {}, {}\n",
    "        for layer_index in range(len(size)-1):\n",
    "            W[layer_index+1] = np.random.randn(size[layer_index+1], size[layer_index])*0.1\n",
    "            b[layer_index+1] = np.random.randn(size[layer_index+1], 1)*0.1\n",
    "        return W,b\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid_gradient(z):\n",
    "        return MultilayerPerceptron.sigmoid(z)*(1 - MultilayerPerceptron.sigmoid(z))\n",
    "\n",
    "    \n",
    "#################################################################################################\n",
    "# 读入图片数据\n",
    "data = pd.read_csv(r\"D:\\DeskTop\\Code\\MyPython\\西瓜书\\3-线性模型\\data\\digits.csv\")\n",
    "        \n",
    "# 数据预处理\n",
    "train_data = data.sample(frac=0.8)\n",
    "test_data = data.sample(10000).values.T\n",
    "\n",
    "num_training_examples = 6000\n",
    "\n",
    "x_train = train_data.iloc[:num_training_examples, 1:].values.T\n",
    "y_train = train_data.iloc[:num_training_examples, 0].values\n",
    "\n",
    "# 训练模型\n",
    "size = [784, 40, 10]\n",
    "max_iterations = 100\n",
    "eta = 1\n",
    "\n",
    "multilayer_perceptron = MultilayerPerceptron(x_train, y_train, size, normlized=True)\n",
    "multilayer_perceptron.train(epoch=max_iterations, eta=eta, \n",
    "                            batch_size=200, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ab637-bb77-4689-977d-b944d9905e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
